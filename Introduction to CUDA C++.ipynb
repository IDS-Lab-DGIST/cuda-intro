{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af03d63",
   "metadata": {},
   "source": [
    "# Introduction to CUDA C++ - IDSLab Seminar [11/03/2022]\n",
    "\n",
    "A simple introduction to CUDA C++. Knowledge of C++ and memory allocation is recommended as CUDA C++ uses pointers, variable addresses and memory allocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0835bde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"slides.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8a2021c7c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"slides.pdf\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac1284",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "In CUDA C++ we program the CPU code and GPU code (Kernel) together\n",
    "```cpp\n",
    "#include <stdio.h> //C++ standard library for input and output, we need printf\n",
    "\n",
    "__global__ void GPUKernel(){//Notice __global__? We are declaring this as a kernel\n",
    "\n",
    "  printf(\"Hi, this is GPU.\\n\");\n",
    "}\n",
    "\n",
    "int main(){ //This will run on CPU\n",
    "\n",
    "  printf(\"Hello, this is CPU.\\n\");\n",
    "\n",
    "  GPUKernel<<<1, 1>>>(); //Execute kernel on GPU\n",
    "  cudaDeviceSynchronize(); //Synchronize GPU threads\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68600723",
   "metadata": {},
   "source": [
    "### Compiling CUDA C++\n",
    "\n",
    "We use `nvcc` to compile CUDA C++. `nvcc` is included with CUDA ToolKit so every GPU server has `nvcc`.\n",
    "\n",
    "CUDA C++ files end with `.cu` extenstion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "280b352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is CPU.\n",
      "Hi, this is GPU.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o hello hello.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b048f",
   "metadata": {},
   "source": [
    "### Let's break it down!\n",
    "#### GPU Code (Kernel)\n",
    "```cpp\n",
    "__global__ void GPUKernel(){//Notice __global__? We are declaring this as a kernel\n",
    "\n",
    "  printf(\"Hi, this is GPU.\\n\");\n",
    "}\n",
    "```\n",
    "`__global__` declares the function is a GPU kernel. But we must call the kernel from the CPU to execute it on GPU.\n",
    "\n",
    "Warning!: Never print in a kernel. This is just an example to show how kernels work. Printing in kernel will greatly hinder the GPU performance and can lead to many bugs.\n",
    "\n",
    "#### CPU Code\n",
    "```cpp\n",
    "int main(){ //This will run on CPU\n",
    "\n",
    "  printf(\"Hello, this is CPU.\\n\");\n",
    "\n",
    "  GPUKernel<<<1, 1>>>(); //Execute kernel on GPU\n",
    "  cudaDeviceSynchronize(); //Synchronize GPU threads\n",
    "}\n",
    "```\n",
    "We use `functionName<<<1, 1>>>()` to execute the kernel. `1, 1` is blocks and threads respectively, we can leave as 1 for  now. CUDA will automatically deploy the kernel to each SM.\n",
    "\n",
    "We must call `cudaDeviceSynchronize()` before starting a new task. The  CPU and GPU run simultaneously, so we must manually tell the CPU to wait for all GPU treads to finish execution.\n",
    "\n",
    "(Advanced Tip: Threads and blocks can be multidimensional and must be set appropriately for the task and data for optimal performance.)\n",
    "\n",
    "(Advanced Tip: If we are running asynchronous tasks, we don't need to call `cudaDeviceSynchronize()`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50d790",
   "metadata": {},
   "source": [
    "## Confused? Let's try a \"small\" numerical example - addition of two 1D arrays\n",
    "\n",
    "Firstly, let's consider how we would add two 1D arrays together using the only the CPU.\n",
    "\n",
    "### CPU Implementation\n",
    "\n",
    "Let's have three arrays  `a`, `b`, and `c`, of equal length, `N`. Where `a` and `b` are the two arrays we want to sum and `c` is the result array.\n",
    "\n",
    "```cpp\n",
    "for(int i = 0; i < N; i++){\n",
    "    c[i] = a[i] + b[i]\n",
    "}\n",
    "\n",
    "```\n",
    "Simple right? But each element of the arrays is summed one by one, so this process is serial.\n",
    "\n",
    "What if we could add all the elements simultaneously? We can, using the parallel threads on a GPU! \n",
    "\n",
    "\n",
    "(Let's ignore CPU multithreading, as the available threads on a CPU [AMD Threadripper PRO 3995WX = 64 cores/128 threads] compared to a GPU [NVIDIA A100 = 6912 cores/threads] is orders of magnitude smaller.)\n",
    "\n",
    "### CUDA Implementation\n",
    "\n",
    "```cpp\n",
    "#include <stdio.h> //C++ standard library for input and ouput, we need printf\n",
    "\n",
    "#define N 100 //C++ define tells the compiler to replace all N to 100 before compiling\n",
    "\n",
    "__global__ void addArrays(int *a, int *b, int *c){ //Notice __global__? We are declaring this as a kernel\n",
    "\n",
    "  int idx = threadIdx.x; //threadIdx allows the kernel to identify what thread it is in\n",
    "  c[idx] = a[idx] + b[idx];\n",
    "}\n",
    "\n",
    "void initArray(int *array, int num){ //Helper function - set every element of the array to a value\n",
    "\n",
    "  for(int i = 0; i < N; ++i){\n",
    "    array[i] = num;\n",
    "  }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "  int *a; //Create a pointer for an int for first array\n",
    "  int *b; //Create a pointer for an int for second array\n",
    "  int *c; //Create a pointer for an int for result array\n",
    "\n",
    "  size_t size = N * sizeof(int); //Calculate the memory size of an int array with length N\n",
    "\n",
    "  //Allocating memory\n",
    "  //cudaMallocManaged will allocate memory on both the CPU DRAM and GPU memory\n",
    "  //cudaMallocManaged needs the memory address (&variable) and size to be allocated\n",
    "  cudaMallocManaged(&a, size);\n",
    "  cudaMallocManaged(&b, size);\n",
    "  cudaMallocManaged(&c, size);\n",
    "\n",
    "  //Initialize our arrays, let's just set every value in the array to one value for now\n",
    "  initArray(a, 10);\n",
    "  initArray(b, 32);\n",
    "  initArray(c, 0);\n",
    "\n",
    "  //Execute our kernel with our arrays that we have initialized\n",
    "  addArrays<<<1, N>>>(a, b, c);\n",
    "  cudaDeviceSynchronize(); //Wait for all threads to be executed\n",
    "\n",
    "  //Let's check every value in the array has summed correctly\n",
    "  for(int i = 0; i < N; i++){\n",
    "    if(c[i] != 42){\n",
    "      printf(\"FAIL: array[%d] - %d does not equal %d\\n\", i, c[i], 42);\n",
    "      exit(1);\n",
    "    }\n",
    "  }\n",
    "  printf(\"Success! All values calculated correctly.\\n\");\n",
    "\n",
    "  //Time to release the memory\n",
    "  cudaFree(a);\n",
    "  cudaFree(b);\n",
    "  cudaFree(c);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411ef1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector_add vector_add.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9523c",
   "metadata": {},
   "source": [
    "### Lets break it down!\n",
    "#### GPU Code (Kernel)\n",
    "\n",
    "```cpp\n",
    "__global__ void addArrays(int *a, int *b, int *c){ //Notice __global__? We are declaring this as a kernel\n",
    "\n",
    "  int idx = threadIdx.x; //threadIdx allows the kernel to identify what thread it is in\n",
    "  c[idx] = a[idx] + b[idx];\n",
    "}\n",
    "```\n",
    "\n",
    "You many of noticed our kernel always returns `void`. The kernel never returns any variables, it works directly on the variables given via pointers. This is why we must allocate the memory for the results array before executing the kernel.\n",
    "\n",
    "`threadIdx` is an object provided CUDA C++ and it is multidemnsion up to three dimensions (`x`, `y` and `z`). `threadIdx` allows the kernel to identifiy which thread index it is running on. As we are doing a single 1D problem we only need `threadIdx.x`. \n",
    "\n",
    "If our array has a length of 100 we can set the number of threads to 100 so there is one thread per element. Then we can simply use the `threadIdx` to sum each element together. (We dont need the for loop like the CPU implemnetation.)\n",
    "\n",
    "\n",
    "#### CPU Code\n",
    "```cpp\n",
    "int main(){\n",
    "  int *a; //Create a int pointer for the first array\n",
    "  int *b; //Create a int pointer for the second array\n",
    "  int *c; //Create a int pointer for the result array\n",
    "\n",
    "  size_t size = N * sizeof(int); //Calculate the memory size of an int array with length N\n",
    "\n",
    "  //Allocating memory\n",
    "  //cudaMallocManaged will allocate memory on both the CPU DRAM and GPU memory\n",
    "  //cudaMallocManaged needs the memory address (&variable) and size to be allocated\n",
    "  cudaMallocManaged(&a, size);\n",
    "  cudaMallocManaged(&b, size);\n",
    "  cudaMallocManaged(&c, size);\n",
    "\n",
    "  //Initialize our arrays, let's just set every value in the array to one value for now\n",
    "  initArray(a, 10);\n",
    "  initArray(b, 32);\n",
    "  initArray(c, 0);\n",
    "\n",
    "  //Execute our kernel with our arrays that we have initialized\n",
    "  addArrays<<<1, N>>>(a, b, c);\n",
    "  cudaDeviceSynchronize(); //Wait for all threads to be executed\n",
    "\n",
    "  //Let's check every value in the array has summed correctly\n",
    "  for(int i = 0; i < N; i++){\n",
    "    if(c[i] != 42){\n",
    "      printf(\"FAIL: array[%d] - %d does not equal %d\\n\", i, c[i], 42);\n",
    "      exit(1);\n",
    "    }\n",
    "  }\n",
    "  printf(\"Success! All values calculated correctly.\\n\");\n",
    "\n",
    "  //Time to release the memory\n",
    "  cudaFree(a);\n",
    "  cudaFree(b);\n",
    "  cudaFree(c);\n",
    "}\n",
    "```\n",
    "\n",
    "This time the CPU code is more complex because we need to manage memory allocation. `cudaMallocManaged()` allows us to allocate CPU and GPU memory and automatically sync between the two. If we update a variable in the GPU kernel, it is also updated in the CPU memory (unified memory). This allows us to easily perform task on data. If we only want to allocate on GPU memory, we would use `cudaMalloc()`.\n",
    "\n",
    "Before using `cudaMallocManaged()` we must declare a pointer to the variable type we want to use. Here we use `int *a;` as we are working with integers. We also need to know the size the array will consume in the memory `size_t size = N * sizeof(int);` we are creating an array of integers with length N. Now we have the pointers for each array and the size we can allocate the memory using `cudaMallocManaged(&a, size)`. `&` before a variable in C++ returns the memory address of the variable. \n",
    "\n",
    "Now we can initialize the arrays with some integers using `initArray()` helper function.\n",
    "\n",
    "Let's execute our `addArrays()` kernel using `addArrays<<<1, N>>>(a, b, c)`. Here we have set `<<<1, N>>>` this will deploy our kernel with N threads, one thread for each element in the arrays.\n",
    "\n",
    "Next `cudaDeviceSynchronize()`, to ensure all threads have completed.\n",
    "\n",
    "Let's check the result is correct on the CPU by checking every element is equal to 42 (10+32).\n",
    "\n",
    "One last step before we finish, we must release the memory allocation using `cudaFree()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5f9b0",
   "metadata": {},
   "source": [
    "## Review\n",
    "This is a very basic introduction just to highlight the functionality of CUDA C++. We have learned how to implement GPU kernels and learned the basics of managing GPU memory.\n",
    "\n",
    "CUDA C++ can be powerful when using more advanced features like shared memory.\n",
    "\n",
    "### CUDA C++ Cheat Sheet\n",
    "\n",
    "`__global__` declares the function is a GPU kernel.\n",
    "\n",
    "`functionName<<<1, 1>>>()` execute the kernel, with `1, 1` blocks and threads.\n",
    "\n",
    "`cudaDeviceSynchronize()` synchronize all the threads.\n",
    "\n",
    "`cudaMallocManaged(&a, size)` allocate memory in CPU and GPU.\n",
    "\n",
    "`cudaFree()` release memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
